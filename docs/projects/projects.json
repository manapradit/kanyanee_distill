[
  {
    "path": "projects/python_ml_regression/",
    "title": "Web Click Prediction",
    "description": "Suppose you have started a new blog on Medium. Their Analytics division provides you with a traffic summary of visitors to your site. This datafile consists simply of a list of #hits per hour for the entire previous month (31 days). You would like to get a feel for the popularity of your site. Are more people reading it the longer it’s been active? Has “word of mouth” had any effect, or has interest in it begun to tail off? What kind of traffic can you expect in the future?",
    "author": [],
    "date": "2022-01-20",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n1. Pre-processing: read in and clean the data\r\n2. Visualization: display the data\r\n3. Analysis and discussion: perform simple linear regression on the data\r\nDiscussion\r\n\r\nProgramming Project 1 Data Visualization and Analysis\r\n1. Pre-processing: read in and clean the data\r\nThe datafile (hits.txt) comes as a comma-separated list of values: each line contains the hour of the month and the number of visits that occurred during that hour (e.g. 1,2272). There are 24x31=744 total lines. A quick glance at the file shows that some type of error has prevented the data from being measured and/or recorded at certain times, represented as a ‘nan’ (“not a number”) value in the datafile. You’re going to have to do something to deal with this problem.\r\nI read in the data using the readlines function. There are 8 ‘nan’ (“not a number”) in the number of hits variable. I decided to replace the ‘nan’ with its mean, approximately 1966 clicks. Then, I change the data type from text to integer to perform later calculations. I also decided to use the mean as an interger for the calculation. In addition, I made two separate lists, including the number of hours and the number of hits. This is becasue I will use these lists to calculate the regression.\r\n\r\n# Read data\r\nhits = open('data/project1-hits.txt')\r\ndata = hits.readlines()\r\nprint(data[0:20])\r\n['1,2272\\n', '2,nan\\n', '3,1386\\n', '4,1365\\n', '5,1488\\n', '6,1337\\n', '7,1883\\n', '8,2283\\n', '9,1335\\n', '10,1025\\n', '11,1139\\n', '12,1477\\n', '13,1203\\n', '14,1311\\n', '15,1299\\n', '16,1494\\n', '17,1159\\n', '18,1365\\n', '19,1272\\n', '20,1246\\n']\r\n\r\n\r\n# Clean data\r\n# Find the mean without nan\r\ntotal_hit = 0\r\ncount_nan = 0\r\ncount_non_nan = 0\r\nfor line in data:\r\n    line = line.strip('\\n').split(',')\r\n    if line[1] != 'nan': # Identify the 'nan', find the sum of hits and number of non-nan to calculate the mean\r\n        day_hit = int(line[1])\r\n        total_hit += day_hit\r\n        count_non_nan += 1\r\n    else:\r\n        count_nan += 1\r\n\r\nmean_non_nan = total_hit/count_non_nan\r\nprint('Mean of hits exclude nan days is ', int(mean_non_nan))\r\n\r\n# Create list of the hrs and hits\r\nMean of hits exclude nan days is  1966\r\nlist_data = []\r\nlist_hrs = []\r\nlist_hits = []        \r\nfor line in data:\r\n    line = line.strip('\\n').split(',')\r\n    if line[1] == 'nan':\r\n        line[1] = mean_non_nan\r\n    #print(line[1])\r\n    line[1] = int(line[1]) \r\n    line[0] = int(line[0])\r\n    list_data.append(line)\r\n    list_hrs.append(line[0])\r\n    list_hits.append(line[1])\r\n     \r\nprint('Data: ', list_data[0:10])\r\nData:  [[1, 2272], [2, 1966], [3, 1386], [4, 1365], [5, 1488], [6, 1337], [7, 1883], [8, 2283], [9, 1335], [10, 1025]]\r\nprint('Hrs list: ', list_hrs[0:10])\r\nHrs list:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\nprint('Hits list: ', list_hits[0:10])    \r\nHits list:  [2272, 1966, 1386, 1365, 1488, 1337, 1883, 2283, 1335, 1025]\r\n\r\n2. Visualization: display the data\r\nI use numpy to transform the data from list to array, then I use matplotlib to create a visualisation of the hour (x-axis) and the number of hits (y-axis). The visualisation shows that the visits are increasing the longer the post has been on the site. The number of visitor was steady until 400-500 hours after the blog was posted. Then, the number of visitor repidly increased since about the hour of 600 until the end of the month.\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\narray_hrs = np.array(list_hrs)\r\narray_hits = np.array(list_hits)\r\n\r\nplt.scatter(array_hrs,array_hits)\r\nplt.show()\r\n\r\n\r\n3. Analysis and discussion: perform simple linear regression on the data\r\nThe linear regression was perform by fitting a straight line to the points. The sum fuction was used to find the sum of all values X and Y. I also use for loop to go though the list and obtain the sum of the products and squares of every X and Y. The regression equation is Y = 1007.57 + 2.57 * X.\r\n\r\n# Obtain/calculate using least-squares method.\r\nsum_x = sum(list_hrs) #the sum of all X values\r\nsum_y = sum(list_hits) #the sum of all Y values\r\n\r\nsum_xy = 0\r\nsum_xsq = 0\r\nsum_ysq = 0\r\nn = 0\r\nfor e in list_data:\r\n    xy = e[0]*e[1]\r\n    sum_xy += xy #the sum of the products of each X,Y pair\r\n    x_sq = e[0]**2\r\n    sum_xsq += x_sq #the sum of the squares of every X value\r\n    y_sq = e[1]**2\r\n    sum_ysq += y_sq #the sum of the squares of every Y value\r\n    n += 1\r\nprint('The sum of all X values: ', sum_x, \r\n      '\\nThe sum of all Y values: ', sum_y, \r\n      '\\nThe sum of the products of each X,Y pair: ', sum_xy, \r\n      '\\nThe sum of the squares of every X value: ', sum_xsq, \r\n      '\\nThe sum of the squares of every Y value: ', sum_ysq, \r\n      '\\nThe number of data points: ', n)\r\nThe sum of all X values:  277140 \r\nThe sum of all Y values:  1462953 \r\nThe sum of the products of each X,Y pair:  633282538 \r\nThe sum of the squares of every X value:  137553820 \r\nThe sum of the squares of every Y value:  3429849451 \r\nThe number of data points:  744\r\nslope = ((n*sum_xy)-(sum_x*sum_y))/((n*sum_xsq)-(sum_x**2))\r\nintercept = (sum_y-(slope*sum_x))/n\r\n\r\nprint('\\nslope is: ', slope, '\\nintercept is: ', intercept)\r\n\r\nslope is:  2.573854364776304 \r\nintercept is:  1007.5739265401816\r\nprint('\\nEquation: ', 'Y = ', intercept, '+', slope, '* X')\r\n\r\n# Create a visualization of the regression analysis (i.e. plot the trendline over the scatter plot of the data). \r\n\r\nEquation:  Y =  1007.5739265401816 + 2.573854364776304 * X\r\nval_y = []\r\nfor e in list_data:\r\n    y = intercept+(slope*e[0])\r\n    val_y.append(y)\r\n    \r\nplt.scatter(array_hrs,array_hits, color='deepskyblue')\r\nplt.plot(val_y, linestyle='dashed', color='tomato', linewidth=3, label='Trend Line')\r\nplt.xlabel(\"Hours\")\r\nplt.ylabel(\"Number of Hits\")\r\nplt.rcParams[\"figure.figsize\"] = (20,5)\r\nleg = plt.legend(loc='lower right')\r\nplt.show()\r\n\r\n\r\nAssuming the regression equation accurately captures current and expected visitor behavior, how many visits would you expect at noon on the fifth day of the next month?\r\nAnswer. The number of visits would expect to be about 3200 hits at noon on the fifth day of the next month. So, firstly, I calculated the hour on the fifth day at noon, which is 852. Secondly, the hour number was substituted to calculate by the original equation, Y = 1007.57 + 2.57 * X. However, the result may not be accurate because the prediction line does not seem to fit well with the higher number of hours. Looking at the scatter plot, I think the predicted number of hits for 852 hours should be higher than the result suggested by this simple regression model.\r\n\r\nx_of_noon_fifth_day = 744+(24*4)+12\r\nprint('Hr at Noon on the fifth day of the next month: ', x_of_noon_fifth_day)\r\nHr at Noon on the fifth day of the next month:  852\r\nprint('\\nNumber of visits expected at Noon on the fifth day of the next month \\n  =', \r\n      intercept, '+', slope, '*', x_of_noon_fifth_day, \r\n     '\\n  =', intercept+(slope*x_of_noon_fifth_day))\r\n\r\nNumber of visits expected at Noon on the fifth day of the next month \r\n  = 1007.5739265401816 + 2.573854364776304 * 852 \r\n  = 3200.497845329593\r\n\r\nDiscussion\r\nThis linear regression is very simple, easy to implement, and reduces the overfitting issues. However, the model may underfit the data because the scatter plot demonstrates exponential growth. Therefore, fitting a straight on to the data may not be appropriate in this case, this model does not seem to do well in predicting the higher the number of hour.\r\nThere are multiple suggestions for future work. First, we could normalize the number of hits during the pre-processing before performing the linear regression using squared root or log. The normalization will transform the dataset to a common scale without distorting differences in the ranges of values. Second, non-linear regression methods such as quadratic polynomial regression or exponential regression could be performed. These approaches will allow the model to better fit the data than a simple linear regression. For example below, I perform the polynomial regression, and we can see that the fitted line aligns along the scatter plot better than the simple linear regression. However, we have to be aware that these approaches could be complex to implement and could have overfitting issues. In addition, the higher level of these approaches could neglect 0, which could happen.\r\n\r\nimport numpy\r\nimport matplotlib.pyplot as plt\r\n\r\nx = np.array(list_hrs)\r\ny = np.array(list_hits)\r\n\r\nmymodel = numpy.poly1d(numpy.polyfit(x, y, 4))\r\n\r\nmyline = numpy.linspace(0, 744)\r\n\r\nplt.scatter(x, y, color='deepskyblue')\r\nplt.plot(myline, mymodel(myline), linestyle='dashed', color='Green', linewidth=4, label='Trend Line')\r\nplt.xlabel(\"Hours\")\r\nplt.ylabel(\"Number of Hits\")\r\nplt.rcParams[\"figure.figsize\"] = (20,5)\r\nleg = plt.legend(loc='lower right')\r\nplt.show()\r\n\r\nprint(mymodel)\r\n           4             3           2\r\n8.935e-08 x - 0.0001026 x + 0.04034 x - 5.757 x + 1755\r\n\r\n\r\n\r\n\r\n",
    "preview": "projects/python_ml_regression/distill-preview.png",
    "last_modified": "2022-12-29T01:16:25-05:00",
    "input_file": {}
  },
  {
    "path": "projects/diamonds/",
    "title": "Diamonds Price Prediction",
    "description": "Diamonds take billions of years to form and not all of them survive the long journey. This project aims to estimate the effects on diamond prices. We run regressions using the diamonds dataset, consisting of 53,940 round diamonds and ten variables. The final model use carat, color, and clarity as variables to estimate their causality on diamond price. The model goodness of fit is evaluated by R-Squared of 0.903.",
    "author": [],
    "date": "2021-12-10",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nData Description\r\nData cleaning and preparation\r\nDistribution and normality of each variable\r\n\r\nExploratory Data Analysis (EDA)\r\nDiamond Price, Carat, and Cut\r\nCorrelations\r\nVariance Inflation Factor (VIF)\r\n\r\nModel selection\r\nData division into train data and test data\r\nSubsetting\r\n\r\nRegression Model\r\nConclusion\r\n\r\nData Description\r\nI have found the dataset from Kaggle website (https://www.kaggle.com/shivam2503/diamonds). It can also be downloaded from the R Tidyverse package or Ggplot2 package, called diamonds. The dataset consists of round cut diamonds from Tiffany & Co’s snapshot price list in 2017. The original dataset includes 53,940 samples and ten variables. However, this analysis will drop 20 observations after cleaning the data. There are seven numerical variables and four categorical variables.\r\nvariable\r\nType\r\nDetail\r\nprice\r\nNumeric\r\nPrice in US dollars\r\ncarat\r\nNumeric\r\nWeight of the diamond\r\ncut\r\nOrdinal\r\nQuality of the cut (Fair, Good, Very Good, Premium, Ideal)\r\ncolor\r\nOrdinal\r\nDiamond color, from D (best) to J (worst)\r\nclarity\r\nOrdinal\r\nA measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\r\nx\r\nNumeric\r\nLength in mm\r\ny\r\nNumeric\r\nWidth in mm\r\nz\r\nNumeric\r\nDepth in mm\r\ndepth\r\nNumeric\r\nTotal depth percentage\r\ntable\r\nNumeric\r\nWidth of top of diamond relative to widest point\r\nLet’s look at the diamonds dataset from the R package. We do not need to separately download the raw data file because the data is available in Ggplot2, a subset of the Tidyverse package. If you want to import the data file into R, you can use the read_csv function in the comment below. Then, we can use the glimpse function to look at the raw data.\r\n\r\n\r\ncode\r\n\r\n#diamonds <- read_csv(here::here(\"data\", \"diamonds.csv\")) \r\nglimpse(diamonds)  \r\n\r\nRows: 53,940\r\nColumns: 10\r\n$ carat   <dbl> 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22…\r\n$ cut     <ord> Ideal, Premium, Good, Premium, Good, Very Good, Very…\r\n$ color   <ord> E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J…\r\n$ clarity <ord> SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, …\r\n$ depth   <dbl> 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1…\r\n$ table   <dbl> 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, …\r\n$ price   <int> 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 33…\r\n$ x       <dbl> 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87…\r\n$ y       <dbl> 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78…\r\n$ z       <dbl> 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49…\r\n\r\nData cleaning and preparation\r\nThe diamonds data requires small cleaning because it is very clean and well organized. As I will run regression models, I have decided to convert the three ordinal variables to numeric scales. I also assume that the scale for color and clarity variables is equally scaled across the levels. In addition, I remove observation with value zero in x, y and z variables because it does not make sense if the dimension is zero. The dataset now has 53,920 samples and ten variables.\r\n\r\n\r\ncode\r\n\r\n# use plyr::revalue to convert cut, color, and clarity scales to number\r\ndata <- diamonds\r\ndata$color <- as.numeric(revalue(data$color, c(\"D\"=1, \"E\"=2, \"F\"=3, \"G\"=4, \"H\"=5, \"I\"=6, \"J\"=7)))\r\ndata$clarity <- as.numeric(revalue(data$clarity, c(\"IF\"=8, \"VVS1\"=7, \"VVS2\"=6, \"VS1\"=5, \"VS2\"=4, \"SI1\"=3, \"SI2\"=2, \"I1\"=1)))\r\ndata$cut <- as.numeric(revalue(data$cut, c(\"Fair\"=1, \"Good\"=2, \"Very Good\"=3, \"Premium\"=4, \"Ideal\"=5)))\r\n\r\ndata <- data %>%\r\n  filter(x>0, y>0, z>0) \r\n\r\n#describe(data, ranges = FALSE)\r\nsummary(data)\r\n\r\n     carat             cut            color          clarity     \r\n Min.   :0.2000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \r\n 1st Qu.:0.4000   1st Qu.:3.000   1st Qu.:2.000   1st Qu.:3.000  \r\n Median :0.7000   Median :4.000   Median :4.000   Median :4.000  \r\n Mean   :0.7977   Mean   :3.904   Mean   :3.594   Mean   :4.052  \r\n 3rd Qu.:1.0400   3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:5.000  \r\n Max.   :5.0100   Max.   :5.000   Max.   :7.000   Max.   :8.000  \r\n     depth           table           price             x         \r\n Min.   :43.00   Min.   :43.00   Min.   :  326   Min.   : 3.730  \r\n 1st Qu.:61.00   1st Qu.:56.00   1st Qu.:  949   1st Qu.: 4.710  \r\n Median :61.80   Median :57.00   Median : 2401   Median : 5.700  \r\n Mean   :61.75   Mean   :57.46   Mean   : 3931   Mean   : 5.732  \r\n 3rd Qu.:62.50   3rd Qu.:59.00   3rd Qu.: 5323   3rd Qu.: 6.540  \r\n Max.   :79.00   Max.   :95.00   Max.   :18823   Max.   :10.740  \r\n       y                z        \r\n Min.   : 3.680   Min.   : 1.07  \r\n 1st Qu.: 4.720   1st Qu.: 2.91  \r\n Median : 5.710   Median : 3.53  \r\n Mean   : 5.735   Mean   : 3.54  \r\n 3rd Qu.: 6.540   3rd Qu.: 4.04  \r\n Max.   :58.900   Max.   :31.80  \r\n\r\nDistribution and normality of each variable\r\nIn this section, we will explore the distribution and normality using the plot_normality() function in the Dlookr package. The function provides normality diagnosis of each variable, including distribution of the original data (upper left), Q-Q plot (upper right), and transformation using log and square root (bottom).\r\n\r\n\r\ncode\r\n\r\ndlookr::plot_normality(data)\r\n\r\n\r\n\r\nExploratory Data Analysis (EDA)\r\nPie chart of the Diamond Cut\r\nThe pie chart shows the diamond cut ratio. Overall, the diamond quality cut was excellent, with 40 percent accounting for ideal quality, 26 percent for premium quality, 22 percent for very good quality, 3 percent was fair quality.\r\n\r\n\r\ncode\r\n\r\ndiamonds%>%\r\n  filter(x>0, y>0, z>0)%>%\r\n  group_by(cut)%>%\r\n  dplyr::summarise(count=n())%>%\r\n  mutate(percent_data=paste(as.character(round(count*100/53920),2),\"%\"))%>% \r\n  ggplot(aes(x=\"\",y=count, fill=cut))+\r\n  geom_bar(stat=\"identity\", width=1)+\r\n  coord_polar(\"y\", start=0)+\r\n  scale_fill_brewer(palette=\"Blues\")+\r\n  geom_text(aes(label = percent_data), \r\n            position = position_stack(vjust = 0.5))+\r\n  theme_void()+\r\n  labs(title = \"Pie chart of the Diamond Cut\")\r\n\r\n\r\n\r\nDiamond Frequency by Color\r\nThe bar chart demonstrates the frequency of the color level. The majority of data consist of color level G at 11,284 observations. In addition, 6,674 diamonds have color level D, which is colorless. The lowest frequency is color level J, which is more yellow than other levels.\r\n\r\n\r\ncode\r\n\r\ndiamonds%>%\r\n  filter(x>0, y>0, z>0)%>%\r\n  group_by(color)%>%\r\n  dplyr::summarise(count=n())%>%\r\n  ggplot(aes(x=color,count,y=count, fill=color))+\r\n  geom_bar(stat=\"identity\", colour=\"black\")+\r\n  scale_fill_manual(values=c(\"#ffffff\", \"#fefcf3\", \"#fdfaeb\", \"#fdf8e4\", \"#fcf6dc\", \"#fcf5d4\", \"#fbf3cd\"))+\r\n  labs(title = \"Diamond Frequency by Color\", x=\"Color of Diamond\", y=\"Count\")+\r\n  theme_bw()+\r\n  theme(legend.position = \"none\",\r\n    panel.grid.major.x = element_blank(),\r\n    panel.border = element_blank(),\r\n    axis.ticks.x = element_blank())+\r\n  geom_text(aes(label = count, vjust = -0.4))\r\n\r\n\r\n\r\nDiamond Frequency by Clarity Level\r\nThe lollipop chart presents the data frequency by clarity color. Most diamonds are slightly included, such as VS2, SI1, and SI2 levels. There are 1,790 internally flawless diamonds. The included diamond is the smallest at only 738 observations.\r\n\r\n\r\ncode\r\n\r\ndiamonds%>%\r\n  filter(x>0, y>0, z>0)%>%\r\n  group_by(clarity)%>%\r\n  dplyr::summarise(count=n())%>%\r\n  ggplot(aes(x=clarity, y=count))+\r\n  geom_segment( aes(x=clarity, xend=clarity, y=0, yend=count), color=\"grey\")+\r\n  geom_point( color=\"gold\", size=4, alpha=0.6)+\r\n  theme_light()+\r\n  coord_flip()+\r\n  labs(title=\"Diamond Frequency by Clarity Level\", x=\"Clarity Level\", y=\"Count\")+\r\n  theme(\r\n    panel.grid.major.y = element_blank(),\r\n    panel.border = element_blank(),\r\n    axis.ticks.x = element_blank())+\r\n    geom_text(aes(label = count, vjust = -1))\r\n\r\n\r\n\r\nDiamond Price, Carat, and Cut\r\nLet’s explore more about the relations between price, carat, and cut. The scatter plot shows that the better cut quality with the same carat has a higher price.\r\n\r\n\r\ncode\r\n\r\ndiamonds%>%\r\n  filter(x>0, y>0, z>0)%>%\r\n  ggplot(aes(carat, price, colour = as.factor(cut))) + geom_point()+\r\n  labs(title = \"Scatter plot of Price and Carat by Cut Quality\", x=\"Carat\", y=\"Diamond Price\", fill=\"Cut Quality\")\r\n\r\n\r\ncode\r\n\r\ndiamonds%>%\r\n  filter(x>0, y>0, z>0)%>%\r\n  ggplot(aes(price, carat, fill = as.factor(cut))) + geom_boxplot()+\r\n  labs(title = \"Boxplot of Price and Carat by Cut Quality\", x=\"Diamond Price\", y=\"Carat\", fill=\"Cut Quality\")+\r\n  coord_flip()\r\n\r\n\r\n\r\nBefore moving on to the next step, I want to check all the variable names in the dataset for validation and easily include them in the following sections.\r\n\r\n\r\ncode\r\n\r\ncolnames(data)\r\n\r\n [1] \"carat\"   \"cut\"     \"color\"   \"clarity\" \"depth\"   \"table\"  \r\n [7] \"price\"   \"x\"       \"y\"       \"z\"      \r\n\r\nCorrelations\r\nThe correlation plot below shows relationships between the dependent variable and independent variables. Variable x, y, and z have significantly high positive relations with each other at more than 0.96. All three variables are also highly correlated to the carat variable. In contrast, the relations between the dependent variable with the quality of cut and the diamond depth is very small negative relation. Therefore, I will remove variables x, y, and z to prevent multicollinearity.\r\n\r\n\r\ncode\r\n\r\n# require library(ggstatsplot)\r\nggstatsplot::ggcorrmat(\r\n  data = cor_data,\r\n  type = \"parametric\", # parametric for Pearson, nonparametric for Spearman's correlation\r\n  colors = c(\"darkred\", \"white\", \"steelblue\") # change default colors\r\n)\r\n\r\n\r\n\r\nVariance Inflation Factor (VIF)\r\nUse variance inflation factor (VIF) to check that there are no two or more independent variables that predict each other.\r\n\r\n\r\ncode\r\n\r\nlibrary(regclass)\r\nVIF(lm(price~carat+cut+color+clarity+depth+table+x+y+z, data=data))\r\n\r\n    carat       cut     color   clarity     depth     table         x \r\n24.838037  1.483701  1.120603  1.230140  1.807764  1.580987 64.340380 \r\n        y         z \r\n20.733711 29.940364 \r\n\r\ncode\r\n\r\nVIF(lm(price~carat+cut+color+clarity+depth+table, data=data))\r\n\r\n   carat      cut    color  clarity    depth    table \r\n1.297629 1.482007 1.118782 1.200325 1.320529 1.576594 \r\n\r\nModel selection\r\nData division into train data and test data\r\nWe will randomly divide the data into two groups: train and test data. First, we set the seed to ensure that we will get the same random result after running the code and the same interpretation. Seventy percent of the data or 37744 observations is the train data. The train data will be used to predict the model, and the test data will be used to fit the final model to observe the model accuracy.\r\n\r\n\r\ncode\r\n\r\ndata2 <- data %>% \r\n  dplyr::select(price,carat,cut,color,clarity,depth,table,x,y,z)\r\n\r\n# split data for training data and test data\r\nset.seed(789) # set seed to gain the same random pattern\r\ntrain = data2 %>%\r\nsample_frac(0.7)\r\ntest = data2 %>%\r\nsetdiff(train)\r\n\r\n\r\nSubsetting\r\nThe below charts illustrate the different values of \\(BIC\\), \\(C_{p}\\), \\(R^2\\), and \\(adjusted R^2\\), regarding the different number of variables in the model. According to the charts, adding depth and table variables have almost no contribution to the model. Hence, we will not consider adding depth and table variables to the model. In addition, adding the quality of diamond cut in the model appear small change to the model. So, I will compare the model containing 4 variables (carat, cut, color, and clarity) and 3 variables (carat, color, and clarity).\r\n\r\n\r\ncode\r\n\r\n# require library(leaps)\r\nregfit_full = regsubsets(price ~ carat+cut+color+clarity+depth+table, data=train)\r\nsummary(regfit_full)\r\n\r\nSubset selection object\r\nCall: regsubsets.formula(price ~ carat + cut + color + clarity + depth + \r\n    table, data = train)\r\n6 Variables  (and intercept)\r\n        Forced in Forced out\r\ncarat       FALSE      FALSE\r\ncut         FALSE      FALSE\r\ncolor       FALSE      FALSE\r\nclarity     FALSE      FALSE\r\ndepth       FALSE      FALSE\r\ntable       FALSE      FALSE\r\n1 subsets of each size up to 6\r\nSelection Algorithm: exhaustive\r\n         carat cut color clarity depth table\r\n1  ( 1 ) \"*\"   \" \" \" \"   \" \"     \" \"   \" \"  \r\n2  ( 1 ) \"*\"   \" \" \" \"   \"*\"     \" \"   \" \"  \r\n3  ( 1 ) \"*\"   \" \" \"*\"   \"*\"     \" \"   \" \"  \r\n4  ( 1 ) \"*\"   \"*\" \"*\"   \"*\"     \" \"   \" \"  \r\n5  ( 1 ) \"*\"   \"*\" \"*\"   \"*\"     \"*\"   \" \"  \r\n6  ( 1 ) \"*\"   \"*\" \"*\"   \"*\"     \"*\"   \"*\"  \r\n\r\ncode\r\n\r\nplot(regfit_full, scale=\"bic\", col=c(\"#51e2f5\",\"#9df9ef\", \"#a28089\"))\r\n\r\n\r\ncode\r\n\r\nplot(regfit_full, scale=\"Cp\", col=c(\"#51e2f5\",\"#9df9ef\", \"#a28089\"))\r\n\r\n\r\ncode\r\n\r\nplot(regfit_full, scale=\"r2\", col=c(\"#83af9b\",\"#c8c8a9\", \"#f9cdad\"))\r\n\r\n\r\ncode\r\n\r\nplot(regfit_full, scale=\"adjr2\", col=c(\"#83af9b\",\"#c8c8a9\", \"#f9cdad\"))\r\n\r\n\r\n\r\nRegression Model\r\nModel with 3 variables:\r\nprice = -4075.927 + 8746.191 x carat - 319.215 x color + 536.701 x clarity\r\nThe estimated effect of carat on diamond price is 8746, the estimated effect of color is -319, and the estimated effect of clarity is 537.\r\n\r\n\r\ncode\r\n\r\nfit_lm3 <- lm(price ~ carat+color+clarity, data = train)\r\nknitr::kable(fit_lm3 %>% broom::tidy())\r\n\r\n\r\nterm\r\n\r\n\r\nestimate\r\n\r\n\r\nstd.error\r\n\r\n\r\nstatistic\r\n\r\n\r\np.value\r\n\r\n\r\n(Intercept)\r\n\r\n\r\n-4075.9265\r\n\r\n\r\n25.598245\r\n\r\n\r\n-159.22680\r\n\r\n\r\n0\r\n\r\n\r\ncarat\r\n\r\n\r\n8746.1914\r\n\r\n\r\n15.228347\r\n\r\n\r\n574.33625\r\n\r\n\r\n0\r\n\r\n\r\ncolor\r\n\r\n\r\n-319.2149\r\n\r\n\r\n3.972704\r\n\r\n\r\n-80.35203\r\n\r\n\r\n0\r\n\r\n\r\nclarity\r\n\r\n\r\n536.7005\r\n\r\n\r\n4.195932\r\n\r\n\r\n127.90972\r\n\r\n\r\n0\r\n\r\n\r\ncode\r\n\r\nknitr::kable(fit_lm3 %>% broom::augment()%>%slice(1:10))\r\n\r\n\r\nprice\r\n\r\n\r\ncarat\r\n\r\n\r\ncolor\r\n\r\n\r\nclarity\r\n\r\n\r\n.fitted\r\n\r\n\r\n.resid\r\n\r\n\r\n.hat\r\n\r\n\r\n.sigma\r\n\r\n\r\n.cooksd\r\n\r\n\r\n.std.resid\r\n\r\n\r\n3985\r\n\r\n\r\n0.93\r\n\r\n\r\n5\r\n\r\n\r\n3\r\n\r\n\r\n4072.0588\r\n\r\n\r\n-87.05878\r\n\r\n\r\n0.0000575\r\n\r\n\r\n1241.250\r\n\r\n\r\n1.00e-07\r\n\r\n\r\n-0.0701410\r\n\r\n\r\n1046\r\n\r\n\r\n0.31\r\n\r\n\r\n4\r\n\r\n\r\n7\r\n\r\n\r\n1115.4371\r\n\r\n\r\n-69.43714\r\n\r\n\r\n0.0001195\r\n\r\n\r\n1241.250\r\n\r\n\r\n1.00e-07\r\n\r\n\r\n-0.0559454\r\n\r\n\r\n2959\r\n\r\n\r\n0.70\r\n\r\n\r\n3\r\n\r\n\r\n5\r\n\r\n\r\n3772.2656\r\n\r\n\r\n-813.26557\r\n\r\n\r\n0.0000391\r\n\r\n\r\n1241.243\r\n\r\n\r\n4.20e-06\r\n\r\n\r\n-0.6552205\r\n\r\n\r\n4876\r\n\r\n\r\n1.01\r\n\r\n\r\n5\r\n\r\n\r\n3\r\n\r\n\r\n4771.7541\r\n\r\n\r\n104.24590\r\n\r\n\r\n0.0000562\r\n\r\n\r\n1241.250\r\n\r\n\r\n1.00e-07\r\n\r\n\r\n0.0839881\r\n\r\n\r\n630\r\n\r\n\r\n0.30\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\n-480.3972\r\n\r\n\r\n1110.39722\r\n\r\n\r\n0.0000935\r\n\r\n\r\n1241.236\r\n\r\n\r\n1.87e-05\r\n\r\n\r\n0.8946337\r\n\r\n\r\n462\r\n\r\n\r\n0.26\r\n\r\n\r\n5\r\n\r\n\r\n5\r\n\r\n\r\n-714.4884\r\n\r\n\r\n1176.48837\r\n\r\n\r\n0.0000998\r\n\r\n\r\n1241.235\r\n\r\n\r\n2.24e-05\r\n\r\n\r\n0.9478856\r\n\r\n\r\n734\r\n\r\n\r\n0.31\r\n\r\n\r\n2\r\n\r\n\r\n5\r\n\r\n\r\n680.4658\r\n\r\n\r\n53.53422\r\n\r\n\r\n0.0000689\r\n\r\n\r\n1241.250\r\n\r\n\r\n0.00e+00\r\n\r\n\r\n0.0431314\r\n\r\n\r\n2936\r\n\r\n\r\n0.74\r\n\r\n\r\n3\r\n\r\n\r\n4\r\n\r\n\r\n3585.4127\r\n\r\n\r\n-649.41268\r\n\r\n\r\n0.0000298\r\n\r\n\r\n1241.245\r\n\r\n\r\n2.00e-06\r\n\r\n\r\n-0.5232074\r\n\r\n\r\n1294\r\n\r\n\r\n0.44\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n2136.6855\r\n\r\n\r\n-842.68552\r\n\r\n\r\n0.0000982\r\n\r\n\r\n1241.242\r\n\r\n\r\n1.13e-05\r\n\r\n\r\n-0.6789432\r\n\r\n\r\n4201\r\n\r\n\r\n0.91\r\n\r\n\r\n5\r\n\r\n\r\n4\r\n\r\n\r\n4433.8355\r\n\r\n\r\n-232.83550\r\n\r\n\r\n0.0000447\r\n\r\n\r\n1241.249\r\n\r\n\r\n4.00e-07\r\n\r\n\r\n-0.1875882\r\n\r\n\r\ncode\r\n\r\nknitr::kable(fit_lm3 %>% broom::glance())\r\n\r\n\r\nr.squared\r\n\r\n\r\nadj.r.squared\r\n\r\n\r\nsigma\r\n\r\n\r\nstatistic\r\n\r\n\r\np.value\r\n\r\n\r\ndf\r\n\r\n\r\nlogLik\r\n\r\n\r\nAIC\r\n\r\n\r\nBIC\r\n\r\n\r\ndeviance\r\n\r\n\r\ndf.residual\r\n\r\n\r\nnobs\r\n\r\n\r\n0.9030269\r\n\r\n\r\n0.9030192\r\n\r\n\r\n1241.233\r\n\r\n\r\n117146.7\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n-322437.4\r\n\r\n\r\n644884.8\r\n\r\n\r\n644927.5\r\n\r\n\r\n58144503086\r\n\r\n\r\n37740\r\n\r\n\r\n37744\r\n\r\n\r\ncode\r\n\r\nres_function <- function(mod){\r\n  par(mfrow = c(2, 2))  \r\nplot(mod)\r\n}\r\n\r\nres_function(fit_lm3)\r\n\r\n\r\n\r\nModel with 4 variables:\r\nprice = -4974.997 + 8776.430 x carat + 165.511 x cut - 318.820 x color + 518.648 x clarity\r\nThe estimated effects of carat, cut, color, and clarity on diamond price are 8776, 165, -318, and 518, respectively. Hence, 1 unit increase in clarity will result in 518 dollars in diamond price.\r\n\r\n\r\ncode\r\n\r\nfit_lm4 <- lm(price ~ carat+cut+color+clarity, data = train)\r\nknitr::kable(fit_lm4 %>% broom::tidy())\r\n\r\n\r\nterm\r\n\r\n\r\nestimate\r\n\r\n\r\nstd.error\r\n\r\n\r\nstatistic\r\n\r\n\r\np.value\r\n\r\n\r\n(Intercept)\r\n\r\n\r\n-4674.9974\r\n\r\n\r\n32.875915\r\n\r\n\r\n-142.20129\r\n\r\n\r\n0\r\n\r\n\r\ncarat\r\n\r\n\r\n8776.4296\r\n\r\n\r\n15.103499\r\n\r\n\r\n581.08586\r\n\r\n\r\n0\r\n\r\n\r\ncut\r\n\r\n\r\n165.5113\r\n\r\n\r\n5.791472\r\n\r\n\r\n28.57845\r\n\r\n\r\n0\r\n\r\n\r\ncolor\r\n\r\n\r\n-318.8205\r\n\r\n\r\n3.930478\r\n\r\n\r\n-81.11494\r\n\r\n\r\n0\r\n\r\n\r\nclarity\r\n\r\n\r\n518.6481\r\n\r\n\r\n4.199093\r\n\r\n\r\n123.51433\r\n\r\n\r\n0\r\n\r\n\r\ncode\r\n\r\nknitr::kable(fit_lm4 %>% broom::augment()%>%slice(1:10))\r\n\r\n\r\nprice\r\n\r\n\r\ncarat\r\n\r\n\r\ncut\r\n\r\n\r\ncolor\r\n\r\n\r\nclarity\r\n\r\n\r\n.fitted\r\n\r\n\r\n.resid\r\n\r\n\r\n.hat\r\n\r\n\r\n.sigma\r\n\r\n\r\n.cooksd\r\n\r\n\r\n.std.resid\r\n\r\n\r\n3985\r\n\r\n\r\n0.93\r\n\r\n\r\n5\r\n\r\n\r\n5\r\n\r\n\r\n3\r\n\r\n\r\n4276.4804\r\n\r\n\r\n-291.48038\r\n\r\n\r\n0.0000915\r\n\r\n\r\n1228.048\r\n\r\n\r\n1.00e-06\r\n\r\n\r\n-0.2373664\r\n\r\n\r\n1046\r\n\r\n\r\n0.31\r\n\r\n\r\n4\r\n\r\n\r\n4\r\n\r\n\r\n7\r\n\r\n\r\n1062.9956\r\n\r\n\r\n-16.99564\r\n\r\n\r\n0.0001218\r\n\r\n\r\n1228.049\r\n\r\n\r\n0.00e+00\r\n\r\n\r\n-0.0138406\r\n\r\n\r\n2959\r\n\r\n\r\n0.70\r\n\r\n\r\n3\r\n\r\n\r\n3\r\n\r\n\r\n5\r\n\r\n\r\n3601.8162\r\n\r\n\r\n-642.81617\r\n\r\n\r\n0.0000627\r\n\r\n\r\n1228.044\r\n\r\n\r\n3.40e-06\r\n\r\n\r\n-0.5234685\r\n\r\n\r\n4876\r\n\r\n\r\n1.01\r\n\r\n\r\n5\r\n\r\n\r\n5\r\n\r\n\r\n3\r\n\r\n\r\n4978.5947\r\n\r\n\r\n-102.59474\r\n\r\n\r\n0.0000909\r\n\r\n\r\n1228.049\r\n\r\n\r\n1.00e-07\r\n\r\n\r\n-0.0835478\r\n\r\n\r\n630\r\n\r\n\r\n0.30\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n3\r\n\r\n\r\n-958.2540\r\n\r\n\r\n1588.25397\r\n\r\n\r\n0.0002789\r\n\r\n\r\n1228.022\r\n\r\n\r\n9.33e-05\r\n\r\n\r\n1.2935125\r\n\r\n\r\n462\r\n\r\n\r\n0.26\r\n\r\n\r\n5\r\n\r\n\r\n5\r\n\r\n\r\n5\r\n\r\n\r\n-566.4312\r\n\r\n\r\n1028.43123\r\n\r\n\r\n0.0001176\r\n\r\n\r\n1228.037\r\n\r\n\r\n1.65e-05\r\n\r\n\r\n0.8375118\r\n\r\n\r\n734\r\n\r\n\r\n0.31\r\n\r\n\r\n4\r\n\r\n\r\n2\r\n\r\n\r\n5\r\n\r\n\r\n663.3404\r\n\r\n\r\n70.65957\r\n\r\n\r\n0.0000691\r\n\r\n\r\n1228.049\r\n\r\n\r\n0.00e+00\r\n\r\n\r\n0.0575408\r\n\r\n\r\n2936\r\n\r\n\r\n0.74\r\n\r\n\r\n5\r\n\r\n\r\n3\r\n\r\n\r\n4\r\n\r\n\r\n3765.2478\r\n\r\n\r\n-829.24785\r\n\r\n\r\n0.0000560\r\n\r\n\r\n1228.041\r\n\r\n\r\n5.10e-06\r\n\r\n\r\n-0.6752843\r\n\r\n\r\n1294\r\n\r\n\r\n0.44\r\n\r\n\r\n5\r\n\r\n\r\n1\r\n\r\n\r\n5\r\n\r\n\r\n2288.6081\r\n\r\n\r\n-994.60807\r\n\r\n\r\n0.0001169\r\n\r\n\r\n1228.038\r\n\r\n\r\n1.53e-05\r\n\r\n\r\n-0.8099673\r\n\r\n\r\n4201\r\n\r\n\r\n0.91\r\n\r\n\r\n4\r\n\r\n\r\n5\r\n\r\n\r\n4\r\n\r\n\r\n4454.0886\r\n\r\n\r\n-253.08858\r\n\r\n\r\n0.0000450\r\n\r\n\r\n1228.048\r\n\r\n\r\n4.00e-07\r\n\r\n\r\n-0.2060974\r\n\r\n\r\ncode\r\n\r\nknitr::kable(fit_lm4 %>% broom::glance())\r\n\r\n\r\nr.squared\r\n\r\n\r\nadj.r.squared\r\n\r\n\r\nsigma\r\n\r\n\r\nstatistic\r\n\r\n\r\np.value\r\n\r\n\r\ndf\r\n\r\n\r\nlogLik\r\n\r\n\r\nAIC\r\n\r\n\r\nBIC\r\n\r\n\r\ndeviance\r\n\r\n\r\ndf.residual\r\n\r\n\r\nnobs\r\n\r\n\r\n0.9050811\r\n\r\n\r\n0.905071\r\n\r\n\r\n1228.033\r\n\r\n\r\n89963.22\r\n\r\n\r\n0\r\n\r\n\r\n4\r\n\r\n\r\n-322033.4\r\n\r\n\r\n644078.7\r\n\r\n\r\n644129.9\r\n\r\n\r\n56912825216\r\n\r\n\r\n37739\r\n\r\n\r\n37744\r\n\r\n\r\ncode\r\n\r\nres_function(fit_lm4)\r\n\r\n\r\n\r\nConclusion\r\nThe models consisting of 3 variables and 4 variables are slightly different. The estimated effects of variables on diamonds price are about the same in each model. The R-squared of 3 variable model (\\(R^2\\) = 0.903) is only 2.5 percent less than that of 4 variable model. Both models present high t-value with significant p-value. Therefore, I consider choosing the 3 variable model, including, carat, color, and clarity. This is because the model is simpler and almost as effective as the model with 4 variables.\r\n\r\n\r\n\r\n",
    "preview": "projects/diamonds/distill-preview.png",
    "last_modified": "2022-12-29T01:16:25-05:00",
    "input_file": {}
  },
  {
    "path": "projects/virus_result/",
    "title": "Forecast virus result using deep learning",
    "description": "This project use R to create pipelines for periodic analysis of a medical data, using classification techniques including decision tree, naive Bayes, support vector machine, and artificial neural networks.",
    "author": [],
    "date": "2021-04-20",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nOverview\r\nData\r\nData cleaning and preparation\r\nExploratory Data Analysis\r\nHistogram\r\nBoxplot\r\n\r\nModels\r\nTree model\r\nNaive Bayes Model\r\nSVM Model\r\nANN\r\n\r\nModel Selection\r\nFinal Model\r\nConclusion\r\n\r\nOverview\r\nData mining methods are widely used to identify patterns, rules or associations, and to predict outcomes among numbers of data. The purpose of this project is to build a pipeline for periodic analysis of employee medical data and choose the best classification technique. The employee medical data is randomly generated with 6,781 observations. The process includes merging the data, cleaning the data, processing classification technique, and generating plots. The decision tree was chosen for the final model due to its overall performance. The model is also easy to explain and compare to other classification models.\r\nData\r\nThe employee medical data contains data from 6,781 employees in total which consists of 5,424 training data observations and 1,357 test data observations. The data are from 2 sources, A and B, which consist of 6 variables and 8 variables, respectively. All files have an ID for each employee which uniquely identifies the employee. Thus, employee ID 123 in the A file is the same employee as ID 123 in the B file.\r\nData cleaning and preparation\r\nThis analysis utilizes the R markdown program along with multiple other R packages to do the model analysis. The following are R packages that have been used in this analysis: ggplot2 and dplyr, accompanied with a collection of R packages designed for data mining such as rpart, rpart.plot, e1071, neuralnet, and so on.\r\nTo clean and prepare the data for analysis, I read the dataset into R and merged the 2 source files horizontally by id and atRisk variables. Then, the data cleaning was performed to filter out observations with missing data, noises, and outliers. I excluded the missing and noisy observations due to the small number of mistakes and a concern about the uncertainty in filling in values. Each variable has a closer range to others, so there might be other mistakes, not only the missing and noisy values. After filtering data inside the acceptable values, the outliers were identified and removed if it is above the 75th or below the 25th percentile. Therefore, I decided to use only selected employees with complete data and omitted observations with missing (n=8), noisy (n=11), or outlier values (n=239), which resulted in dropping 259 observations from the training data. In addition, some data types had to be changed from numeric to categorical and adjusted for the appropriate usage.\r\n\r\n\r\ncode\r\n\r\n# read data into R\r\ndataTrainA=read.table(\"data/virus_test_deep_learning/dataTrainA.txt\", header=TRUE)\r\ndataTrainB=read.table(\"data/virus_test_deep_learning/dataTrainB.txt\", header=TRUE)\r\ndataTestA=read.table(\"data/virus_test_deep_learning/dataTestA.txt\", header=TRUE)\r\ndataTestB=read.table(\"data/virus_test_deep_learning/dataTestB.txt\", header=TRUE)\r\n\r\n# Merge train dataset A and B by id and atRisk\r\ntraindata=merge(dataTrainA, dataTrainB, by=c(\"id\",\"atRisk\"))\r\n\r\n# Merge test dataset A and B\r\ntestdata=merge(dataTestA, dataTestB, by=c(\"id\",\"atRisk\"))\r\n\r\n\r\nThe train data has 8 missing values, while the test data has no missing value.\r\n\r\n\r\ncode\r\n\r\ntable(is.na(traindata))\r\n\r\n\r\nFALSE  TRUE \r\n65080     8 \r\n\r\ncode\r\n\r\ntable(is.na(testdata))\r\n\r\n\r\nFALSE \r\n16284 \r\n\r\n\r\n\r\ncode\r\n\r\n#functions to filter noises and missing\r\nmyCleanFunction <- function(datatoclean){\r\n                    datatoclean%>%filter(between(temp,90,106),\r\n                      between(bpSys,97,150),\r\n                      between(vo2,10,70),\r\n                      between(throat,80,120),\r\n                      between(atRisk,0,1),\r\n                      between(headA,0,9),\r\n                      between(bodyA,0,9),\r\n                      between(cough,0,1),\r\n                      between(runny,0,1),\r\n                      between(nausea,0,1),\r\n                      between(diarrhea,0,1)\r\n                    )} \r\n\r\n#function to identify outliers\r\noutliers <- function(data,c){boxplot(data[,c], plot=FALSE)$out}\r\n\r\n\r\nTrain and test data after cleaning Train data\r\n\r\n\r\ncode\r\n\r\nsummary(tr)\r\n\r\n       id           atRisk            temp            bpSys      \r\n Min.   :   0   Min.   :0.0000   Min.   : 96.18   Min.   :103.0  \r\n 1st Qu.:1688   1st Qu.:0.0000   1st Qu.: 97.77   1st Qu.:119.0  \r\n Median :3360   Median :0.0000   Median : 98.16   Median :124.0  \r\n Mean   :3383   Mean   :0.4521   Mean   : 98.40   Mean   :124.4  \r\n 3rd Qu.:5091   3rd Qu.:1.0000   3rd Qu.: 98.80   3rd Qu.:130.0  \r\n Max.   :6780   Max.   :1.0000   Max.   :100.63   Max.   :146.0  \r\n      vo2            throat          headA           bodyA      \r\n Min.   :22.00   Min.   : 88.0   Min.   :0.000   Min.   :1.000  \r\n 1st Qu.:34.00   1st Qu.: 97.0   1st Qu.:3.000   1st Qu.:4.000  \r\n Median :39.00   Median :100.0   Median :3.000   Median :4.000  \r\n Mean   :38.04   Mean   :100.1   Mean   :3.451   Mean   :4.017  \r\n 3rd Qu.:42.00   3rd Qu.:103.0   3rd Qu.:4.000   3rd Qu.:4.000  \r\n Max.   :54.00   Max.   :112.0   Max.   :8.000   Max.   :7.000  \r\n     cough            runny            nausea          diarrhea     \r\n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \r\n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \r\n Median :0.0000   Median :0.0000   Median :0.0000   Median :0.0000  \r\n Mean   :0.3258   Mean   :0.1983   Mean   :0.2403   Mean   :0.1022  \r\n 3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.0000  \r\n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \r\n\r\nTest data\r\n\r\n\r\ncode\r\n\r\nsummary(te)\r\n\r\n       id           atRisk            temp            bpSys      \r\n Min.   :   2   Min.   :0.0000   Min.   : 96.35   Min.   : 98.0  \r\n 1st Qu.:1786   1st Qu.:0.0000   1st Qu.: 97.79   1st Qu.:119.0  \r\n Median :3533   Median :0.0000   Median : 98.21   Median :124.0  \r\n Mean   :3447   Mean   :0.4584   Mean   : 98.51   Mean   :124.4  \r\n 3rd Qu.:5087   3rd Qu.:1.0000   3rd Qu.: 99.12   3rd Qu.:130.0  \r\n Max.   :6770   Max.   :1.0000   Max.   :101.58   Max.   :147.0  \r\n      vo2            throat           headA           bodyA      \r\n Min.   :18.00   Min.   : 86.00   Min.   :0.000   Min.   :2.000  \r\n 1st Qu.:33.00   1st Qu.: 97.00   1st Qu.:3.000   1st Qu.:4.000  \r\n Median :38.00   Median :100.00   Median :3.000   Median :4.000  \r\n Mean   :37.48   Mean   : 99.73   Mean   :3.408   Mean   :4.025  \r\n 3rd Qu.:42.00   3rd Qu.:103.00   3rd Qu.:4.000   3rd Qu.:4.000  \r\n Max.   :55.00   Max.   :114.00   Max.   :8.000   Max.   :7.000  \r\n     cough          runny            nausea          diarrhea      \r\n Min.   :0.00   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  \r\n 1st Qu.:0.00   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000  \r\n Median :0.00   Median :0.0000   Median :0.0000   Median :0.00000  \r\n Mean   :0.35   Mean   :0.1945   Mean   :0.2402   Mean   :0.09359  \r\n 3rd Qu.:1.00   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.00000  \r\n Max.   :1.00   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  \r\n\r\nExploratory Data Analysis\r\nThe exploratory data analysis was performed to develop an understanding of the data, and the table below shows a glance summary of each variable. There are 5 numeric variables such as patient’s temperature (temp), blood pressure (systolic) (bpSys), VO2 max (vo2), and throat culture (throat). There are 6 categorical variables including level of headache, level of body ache, cough, runny nose, nausea, diarrhea, and virus test (atRisk).\r\nTable 1: Table representing variable summary (n=5165).\r\nVariable\r\nType\r\nMedian\r\nMean\r\nSD\r\nMin\r\nMax\r\ntemp\r\nNumeric\r\n98.16\r\n98.4\r\n0.91\r\n96.18\r\n100.63\r\nbpSys\r\nNumeric\r\n124\r\n124.4\r\n8.05\r\n103\r\n146\r\nvo2\r\nNumeric\r\n39\r\n38.04\r\n5.97\r\n22\r\n54\r\nthroat\r\nNumeric\r\n100\r\n100.1\r\n4.47\r\n88\r\n112\r\nheadA\r\nFactor\r\n3\r\n3.45\r\n1.07\r\n0\r\n8\r\nbodyA\r\nFactor\r\n4\r\n4.02\r\n0.67\r\n1\r\n7\r\ncough\r\nFactor\r\n0 (n=3482), 1 (n=1683)\r\n\r\n\r\n\r\n\r\nrunny\r\nFactor\r\n0 (n=4141), 1 (n=1024)\r\n\r\n\r\n\r\n\r\nnausea\r\nFactor\r\n0 (n=3924), 1 (n=1241)\r\n\r\n\r\n\r\n\r\ndiarrhea\r\nFactor\r\n0 (n=4637), 1 (n=528)\r\n\r\n\r\n\r\n\r\natRisk\r\nFactor\r\n0 (n=2830), 1 (n=2335)\r\n\r\n\r\n\r\n\r\nHistogram\r\n\r\n\r\ncode\r\n\r\nhist2 = function(df, var,b){ggplot(df, aes(x=var, color=atRisk))+\r\n    geom_histogram(fill=\"white\", alpha=0.5, position=\"identity\",binwidth = b)+\r\n    theme_minimal()+\r\n    ggtitle(\"Histogram of employee medical data\")}\r\n\r\n\r\n\r\n\r\ncode\r\n\r\nhist2(md,md[,3],0.1)+xlab(\"patient's temperature\")+ggtitle(\"Histrogram of emplyee’s temperature by virus test\")\r\nhist2(md,md[,4],1)+xlab(\"blood pressure (systolic)\")+ggtitle(\"Histrogram of  by virus test\")\r\nhist2(md,md[,5],1)+xlab(\"VO2 max\")+ggtitle(\"Histrogram of VO2 max by virus test\")\r\nhist2(md,md[,6],1)+xlab(\"throat culture\")+ggtitle(\"Histrogram of throat culture by virus test\")\r\nhist2(md,md[,7],1)+xlab(\"headache\")+ggtitle(\"Histrogram of headache by virus test\")\r\n\r\n\r\n\r\nBoxplot\r\n\r\n\r\ncode\r\n\r\nboxplotFunction=function(dat,c,var, title, x, y){\r\n  boxplot(dat[,c]~var, data=dat, main=title,\r\n          xlab=x, ylab=y)}\r\n\r\n\r\n\r\n\r\ncode\r\n\r\nboxplotFunction(tr,3,tr$atRisk,\"Boxplot of patient's temperature by virus test\", \"virus test\", \"patient's temperature\")\r\nboxplotFunction(tr,4,tr$atRisk,\"Boxplot of blood pressure by virus test\", \"virus test\", \"blood pressure\")\r\nboxplotFunction(tr,5,tr$atRisk,\"Boxplot of VO2 max by virus test\", \"virus test\", \"VO2 max \")\r\nboxplotFunction(tr,6,tr$atRisk,\"Boxplot of throat culture by virus test\", \"virus test\", \"throat culture \")\r\n\r\n\r\n\r\nOverall, the distributions vary, but histograms positive and negative virus tests show that there is normal distribution for all the variables except for the patient’s temperature. The histogram that displays the temperature variable is a bi-modal distribution with a higher frequency on the left distribution. The histogram of the temperature variable by the virus test illustrates that the distribution of the negative test group is skewed to the right, while the positive test group is bi-modal. This could be because some people who had the virus are symptomatic or the data was collected during the incubation period. In addition, the boxplot of throat culture for positive and negative virus tests has two similar shapes, so we can expect this variable to have an insignificant impact in this analysis.\r\nModels\r\nThe algorithms used to build the model include decision tree (CART), Naive Bayes, support vector machine (SVM), and neural networks (ANN). All were implemented in R. The simplest form for each classifier was used and parameters were set; for SVM, kernel=polynomial and for ANN, hidden=4. Each model had been divided into four steps. First, prepare the data to use in the model as different models have preferred types of data; for example, ANN model requires numeric data. Second, the model was built from the training data, followed by predicting the outcome using test data. Finally, the accuracy of each model was calculated so I could compare the models’ performance.\r\nTree model\r\nThe tree is build using this equation; atRisk~temp+bpSys+vo2+throat+headA+bodyA+cough+runny+nausea+diarrhea\r\n\r\n\r\ncode\r\n\r\neq=atRisk~temp+bpSys+vo2+throat+headA+bodyA+cough+runny+nausea+diarrhea #equation for models\r\n\r\n\r\n\r\n\r\ncode\r\n\r\n# 1. Prepare Data\r\ntr_tree=tr\r\nte_tree=te\r\n\r\ntr_tree[,2]=as.factor(tr_tree[,2])\r\nfor (i in 7:12){tr_tree[,i] = as.factor(tr_tree[,i])}\r\n\r\nte_tree[,2]=as.factor(te_tree[,2])\r\nfor (i in 7:12){te_tree[,i] = as.factor(te_tree[,i])}\r\n\r\n# 2. Build Model\r\n# use rpart to build a decision tree model using atRisk as the class and all of the attributes except id\r\nt1=Sys.time()\r\nmod_tree=rpart(eq, data=tr_tree)\r\nt2=Sys.time()\r\ntime_treel = as.double(t2-t1)\r\nmod_tree\r\n\r\nn= 5165 \r\n\r\nnode), split, n, loss, yval, (yprob)\r\n      * denotes terminal node\r\n\r\n 1) root 5165 2335 0 (0.5479187 0.4520813)  \r\n   2) bpSys< 126.5 3105  836 0 (0.7307568 0.2692432)  \r\n     4) temp< 99.165 2812  584 0 (0.7923186 0.2076814)  \r\n       8) headA=0,1,2,3,4 2486  335 0 (0.8652454 0.1347546) *\r\n       9) headA=5,6,7 326   77 1 (0.2361963 0.7638037) *\r\n     5) temp>=99.165 293   41 1 (0.1399317 0.8600683) *\r\n   3) bpSys>=126.5 2060  561 1 (0.2723301 0.7276699)  \r\n     6) temp< 99.125 1303  482 1 (0.3699156 0.6300844)  \r\n      12) headA=1,2,3 440   86 0 (0.8045455 0.1954545) *\r\n      13) headA=4,5,6,7,8 863  128 1 (0.1483198 0.8516802) *\r\n     7) temp>=99.125 757   79 1 (0.1043593 0.8956407) *\r\n\r\ncode\r\n\r\n#plot the decision tree using rpart.plot\r\nrpart.plot(mod_tree)\r\n\r\n\r\ncode\r\n\r\n#What is one of the main determiners of voter approval?\r\n\r\n# 3. Predict\r\nt1=Sys.time()\r\npred_tree = predict(mod_tree, te_tree[c(3,4,5,6,7,8,9,10,11,12)], type=\"vector\")\r\nt2=Sys.time()\r\ntime_treep = as.double(t2-t1)\r\n\r\n# 4. Calculate accuracy\r\ntab_tree = table(te_tree$atRisk,pred_tree)\r\ntab_tree\r\n\r\n   pred_tree\r\n      1   2\r\n  0 630 105\r\n  1 101 521\r\n\r\ncode\r\n\r\naccFunction=function(table){sum(diag(table))/sum(table)}\r\n#acc_tree <- sum(diag(tab1))/sum(tab1)\r\nacc_tree=accFunction(tab_tree)\r\nacc_tree\r\n\r\n[1] 0.8481945\r\n\r\nPruning the tree\r\n\r\n\r\ncode\r\n\r\ntrain=tr_tree\r\n\r\n#Base Model\r\nbase_model <- rpart(eq, data = train, method = \"class\",\r\n                       control = rpart.control(cp = 0))\r\n#summary(base_model)\r\n\r\n#Plot Decision Tree\r\nplot(base_model)\r\n\r\n\r\ncode\r\n\r\n# Examine the complexity plot\r\nprintcp(base_model)\r\n\r\n\r\nClassification tree:\r\nrpart(formula = eq, data = train, method = \"class\", control = rpart.control(cp = 0))\r\n\r\nVariables actually used in tree construction:\r\n[1] bodyA  bpSys  headA  nausea runny  temp   throat vo2   \r\n\r\nRoot node error: 2335/5165 = 0.45208\r\n\r\nn= 5165 \r\n\r\n           CP nsplit rel error  xerror     xstd\r\n1  4.0171e-01      0   1.00000 1.00000 0.015318\r\n2  9.0364e-02      1   0.59829 0.60728 0.013736\r\n3  7.3662e-02      2   0.50792 0.56403 0.013415\r\n4  5.7388e-02      3   0.43426 0.43555 0.012239\r\n5  3.8544e-03      5   0.31949 0.32120 0.010844\r\n6  2.9979e-03      7   0.31178 0.31649 0.010777\r\n7  2.8551e-03     10   0.30278 0.31306 0.010728\r\n8  1.7131e-03     13   0.29422 0.30150 0.010560\r\n9  1.2848e-03     15   0.29079 0.30535 0.010617\r\n10 6.4240e-04     20   0.28394 0.30578 0.010623\r\n11 4.2827e-04     22   0.28266 0.30792 0.010654\r\n12 2.5696e-04     28   0.28009 0.31478 0.010753\r\n13 2.1413e-04     34   0.27837 0.32248 0.010862\r\n14 1.6472e-04     42   0.27666 0.32548 0.010903\r\n15 1.0707e-04     55   0.27452 0.33619 0.011050\r\n16 9.5170e-05     59   0.27409 0.34218 0.011130\r\n17 7.1378e-05     68   0.27323 0.34433 0.011158\r\n18 6.1181e-05     74   0.27281 0.35375 0.011281\r\n19 0.0000e+00     81   0.27238 0.35889 0.011347\r\n\r\ncode\r\n\r\nplotcp(base_model)\r\n\r\n\r\ncode\r\n\r\nrpart.plot(base_model)\r\n\r\n\r\ncode\r\n\r\ntest=te_tree\r\n\r\n# Compute the accuracy of the pruned tree\r\ntest$pred <- predict(base_model, te_tree[c(3,4,5,6,7,8,9,10,11,12)], type = \"class\")\r\nbase_accuracy <- mean(test$pred == test$atRisk)\r\n\r\n\r\n\r\n\r\ncode\r\n\r\n# Grow a tree with minsplit of 100 and max depth of 8\r\nmodel_preprun <- rpart(eq, data = train, method = \"class\", \r\n                   control = rpart.control(cp = 0, maxdepth = 8,minsplit = 100))\r\n# Compute the accuracy of the pruned tree\r\ntest$pred <- predict(model_preprun, te_tree[c(3,4,5,6,7,8,9,10,11,12)], type = \"class\")\r\naccuracy_preprun <- mean(test$pred == test$atRisk)\r\n\r\nrpart.plot(model_preprun)\r\n\r\n\r\n\r\n\r\n\r\ncode\r\n\r\n#Postpruning\r\n# Prune the base_model based on the optimal cp value\r\nmodel_pruned <- prune(base_model, cp = 0.0084 )\r\n\r\n# Compute the accuracy of the pruned tree\r\ntest$pred <- predict(model_pruned, te_tree[c(3,4,5,6,7,8,9,10,11,12)], type = \"class\")\r\naccuracy_postprun <- mean(test$pred == test$atRisk)\r\ndata.frame(base_accuracy, accuracy_preprun, accuracy_postprun)\r\n\r\n  base_accuracy accuracy_preprun accuracy_postprun\r\n1     0.8327192        0.8607222         0.8481945\r\n\r\ncode\r\n\r\nrpart.plot(model_pruned)\r\n\r\n\r\n\r\nNaive Bayes Model\r\n\r\n\r\ncode\r\n\r\n# 1. Prepare Data\r\n#https://www.rdocumentation.org/packages/e1071/versions/1.7-6/topics/naiveBayes\r\n#library(e1071)\r\ntr_nb=tr\r\nte_nb=te\r\n\r\ntr_nb[,2]=as.factor(tr_nb[,2])\r\nfor (i in 7:12){tr_nb[,i] = as.factor(tr_nb[,i])}\r\n\r\nte_nb[,2]=as.factor(te_nb[,2])\r\nfor (i in 7:12){te_nb[,i] = as.factor(te_nb[,i])}\r\n\r\n# 2. Build Model\r\nt1=Sys.time()\r\nmod_nb=naiveBayes(eq,tr_nb)\r\nt2=Sys.time()\r\ntime_nbl = as.double(t2-t1)\r\nmod_nb\r\n\r\n\r\nNaive Bayes Classifier for Discrete Predictors\r\n\r\nCall:\r\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\r\n\r\nA-priori probabilities:\r\nY\r\n        0         1 \r\n0.5479187 0.4520813 \r\n\r\nConditional probabilities:\r\n   temp\r\nY       [,1]      [,2]\r\n  0 98.08185 0.6109729\r\n  1 98.78294 1.0465964\r\n\r\n   bpSys\r\nY       [,1]     [,2]\r\n  0 120.8972 6.879693\r\n  1 128.6039 7.299815\r\n\r\n   vo2\r\nY       [,1]     [,2]\r\n  0 39.64346 4.897654\r\n  1 36.08651 6.552161\r\n\r\n   throat\r\nY       [,1]     [,2]\r\n  0 100.0049 4.499163\r\n  1 100.1118 4.432094\r\n\r\n   headA\r\nY              0            1            2            3            4\r\n  0 0.0007067138 0.0215547703 0.1236749117 0.6742049470 0.1257950530\r\n  1 0.0004282655 0.0115631692 0.0745182013 0.3811563169 0.1379014989\r\n   headA\r\nY              5            6            7            8\r\n  0 0.0445229682 0.0084805654 0.0010600707 0.0000000000\r\n  1 0.3250535332 0.0612419700 0.0077087794 0.0004282655\r\n\r\n   bodyA\r\nY              1            2            3            4            5\r\n  0 0.0007067138 0.0166077739 0.1293286219 0.6826855124 0.1424028269\r\n  1 0.0017130621 0.0175588865 0.1336188437 0.6959314775 0.1366167024\r\n   bodyA\r\nY              6            7\r\n  0 0.0257950530 0.0024734982\r\n  1 0.0137044968 0.0008565310\r\n\r\n   cough\r\nY           0         1\r\n  0 0.7720848 0.2279152\r\n  1 0.5554604 0.4445396\r\n\r\n   runny\r\nY           0         1\r\n  0 0.8053004 0.1946996\r\n  1 0.7974304 0.2025696\r\n\r\n   nausea\r\nY           0         1\r\n  0 0.8710247 0.1289753\r\n  1 0.6248394 0.3751606\r\n\r\n   diarrhea\r\nY            0          1\r\n  0 0.89328622 0.10671378\r\n  1 0.90321199 0.09678801\r\n\r\ncode\r\n\r\n# 3. Predict\r\nt1=Sys.time()\r\npred_nb = predict(mod_nb, te_nb[c(3,4,5,6,7,8,9,10,11)])\r\nt2=Sys.time()\r\ntime_nbp = as.double(t2-t1)\r\n\r\n# 4. Calculate accuracy\r\ntab_nb=table(pred_nb,te_nb$atRisk)\r\ntab_nb\r\n\r\n       \r\npred_nb   0   1\r\n      0 639 109\r\n      1  96 513\r\n\r\ncode\r\n\r\nacc_nb =accFunction(tab_nb)\r\nacc_nb\r\n\r\n[1] 0.8489315\r\n\r\nSVM Model\r\n\r\n\r\ncode\r\n\r\n# 1. Prepare Data\r\n#http://uc-r.github.io/svm\r\n#https://www.rdocumentation.org/packages/e1071/versions/1.7-6/topics/plot.svm\r\n#For svm you will have to make them all numeric (integer) except the class.\r\ntr_svm=tr[-c(1)]\r\nte_svm=te[-c(1)]\r\n\r\ntrainsvmdata <- as.data.frame(apply(tr_svm, 2, as.integer)) #make all numeric\r\ntrainsvmdata[,1]=as.factor(tr_svm[,1])   #except class is a factor\r\n\r\ntestsvmdata <- as.data.frame(apply(te_svm, 2, as.integer)) #make all numeric\r\ntestsvmdata[,1]=as.factor(te_svm[,1])   #except class is a factor\r\n\r\n# 2. Build Model\r\n#SVM model 1 linear not working as line cannot divide between atRisk.\r\nt1=Sys.time()\r\nmod_svm1=svm(eq, data=trainsvmdata, kernel=\"linear\")\r\nt2=Sys.time()\r\ntime_svmll = as.double(t2-t1)\r\nmod_svm1\r\n\r\n\r\nCall:\r\nsvm(formula = eq, data = trainsvmdata, kernel = \"linear\")\r\n\r\n\r\nParameters:\r\n   SVM-Type:  C-classification \r\n SVM-Kernel:  linear \r\n       cost:  1 \r\n\r\nNumber of Support Vectors:  2200\r\n\r\ncode\r\n\r\n# 3. Predict\r\nt1=Sys.time()\r\npredsvm1 = predict(mod_svm1,testsvmdata[c(2,3,4,5,6,7,8,9,10,11)])\r\nt2=Sys.time()\r\ntime_svmlp = as.double(t2-t1)\r\n\r\n# 4. Calculate accuracy\r\ntab_svmlin=table(predsvm1,testsvmdata$atRisk)\r\ntab_svmlin\r\n\r\n        \r\npredsvm1   0   1\r\n       0 632 114\r\n       1 103 508\r\n\r\ncode\r\n\r\nacc_svm1=accFunction(tab_svmlin)\r\nacc_svm1\r\n\r\n[1] 0.8400884\r\n\r\ncode\r\n\r\n# 2.2 Build Model SVM with  kernel=\"polynomial\"\r\n#SVM model 2\r\nt1=Sys.time()\r\nmod_svm2=svm(eq, data=trainsvmdata, kernel=\"polynomial\")\r\nt2=Sys.time()\r\ntime_svmpl = as.double(t2-t1)\r\nmod_svm2\r\n\r\n\r\nCall:\r\nsvm(formula = eq, data = trainsvmdata, kernel = \"polynomial\")\r\n\r\n\r\nParameters:\r\n   SVM-Type:  C-classification \r\n SVM-Kernel:  polynomial \r\n       cost:  1 \r\n     degree:  3 \r\n     coef.0:  0 \r\n\r\nNumber of Support Vectors:  2128\r\n\r\ncode\r\n\r\n# 3.2 Predict\r\nt1=Sys.time()\r\npredsvm2 = predict(mod_svm2,testsvmdata[c(2,3,4,5,6,7,8,9,10,11)])\r\nt2=Sys.time()\r\ntime_svmpp = as.double(t2-t1)\r\n\r\n# 4.2 Calculate accuracy\r\ntab_svm2=table(predsvm2,testsvmdata$atRisk)\r\ntab_svm2\r\n\r\n        \r\npredsvm2   0   1\r\n       0 656 128\r\n       1  79 494\r\n\r\ncode\r\n\r\nacc_svm2=accFunction(tab_svm2)\r\nacc_svm2\r\n\r\n[1] 0.8474576\r\n\r\nANN\r\nModel Selection\r\nAfter running the models in R markdown which is provided in the appendix, we obtain the following results as shown in table 2 below. ANN has the highest accuracy among these classifications at 0.862, followed by Naive Bayes, decision tree, and support vector machine (SVM) with accuracy rates of 0.849, 0.848, and 0.847, respectively. Unsurprisingly, ANN is the most accurate in this analysis because ANN is more complex than other models. This is also because this analysis set the hidden nodes equal to 4, but the accuracy decreases to almost the same as other techniques when using hidden nodes equal to 3.\r\nThen, I used learning time and predicting time to compare the model expensiveness. The learning time is the time that the model spends on training the model. The predicting time is the time that the model uses to predict the class of the test dataset. The decision tree has the least predicting time at 5.00 seconds, with second lowest learning time at 1.02 seconds. Although, Naive Bayes has the least learning time at 0.78 seconds, it has high predicting time at 16.46 seconds. Unfortunately, ANN’s learning time is extremely high (32 minutes) because the high accuracy of ANN is compensated by the time that ANN spends learning the complex model.\r\nAccording to the small difference in accuracy and the model expensiveness, I decided to use the decision tree technique for further analysis. Even though the tree has limitations over accuracy and learning time in this test, there are advantages compared to other techniques. The decision tree model is easy to understand and interpret with simple visualization, which allows quick prediction using the condition in the tree diagram. Decision tree can handle collinearity efficiently. It is less complex than ANN and SVM but still flexible than Naive Bayes.\r\nTable 2: Table representing the model’s effectiveness using training data = 5,165 and test data = 1,357\r\nModel\r\nAccuracy\r\nLearning Time (minute)\r\nPredicting Time (Munite)\r\nTree\r\n0.8482\r\n0.08329\r\n0.017\r\nNaive Bayes\r\n0.84893\r\n0.01296\r\n0.27427\r\nSVM\r\n0.84746\r\n1.15642\r\n0.0728\r\nANN\r\n0.8622\r\n32.02853\r\n0.01596\r\n\r\nNote: Bold texts represent the best performance in column.\r\nFinal Model\r\nI chose decision tree for the final model because of its overall performance, and the model is easy to understand by people. In addition, pruning technique was performed to compare the tree performance of the base model, the model before pruning, and the model after pruning. The pruning process involves removing the branches that make use of features having low importance. This reduces the complexity of the tree, hence increasing its predictive power by reducing overfitting. The type of decision tree used in this analysis is Classification And Regression Tree (CART) which shows both prediction outcomes, discrete number of class and a real number of predicted outcome. The colored leaves show the probability of virus test status, and the percentage represents the number of employees in the train data that belong to the group. The bold text in black represents a condition based on which the tree splits into yes or no.\r\nThe tree diagram used 3 variables for prediction including blood pressure, temperature, and headache level. The model suggested that an employee with a temperature of more than 99.1 degree Fahrenheit is very likely to have the virus. An employee is also likely to have the virus if the employee has blood pressure more than 127, temperature less than 99° Fahrenheit, and has the headache level of more than 4. Likewise, an employee with lower blood pressure and temperature with the level of headache below 4 is likely to have a negative virus test.\r\nI think that these 3 variables and conditions used in the tree diagram makes sense. First, blood pleasure is the most predictive attribute in Naive Bayes model, because other attributes have the overlap value between positive and negative virus test. This overlap could also be seen in boxplot that shows the negative and positive virus test. Second, an employee with the temperature of 99 degrees is considered to have the virus in this analysis. The statistics and the histogram above show condition is reasonable. The average temperature of this dataset is about 98 degree with standard deviation of 0.9. The frequency of employees with positive test begins to obviously increased at 99 degree, as well as the frequency for negative virus test start to fall.\r\nFigure 2: Decision tree diagram\r\n\r\n\r\ncode\r\n\r\nrpart.plot(model_pruned)\r\n\r\n\r\n\r\nConclusion\r\nThe pipeline for periodic analysis of employee medical data is built by cleaning the data, building models, predicting test data, and calculating the models’ accuracy. The results show that all the models have accuracy at approximately 0.8, ANN has the extremely high leaning time, and the decision tree uses the least time for the prediction. Thus, the most effective classification technique in this analysis is a decision tree with three variables. This is because of its overall performance, simple visualization, and straightforward interpretation. Moreover, the three variable decision tree is almost as effective as those complex models with many variables. However, the results are different depending on the method used to identify outliers. This model could be helpful in primary screening employees with the risk of having the virus and preventing the virus from spreading through the workplace.\r\n\r\n\r\n\r\n",
    "preview": "projects/virus_result/distill-preview.png",
    "last_modified": "2022-12-29T01:16:25-05:00",
    "input_file": {}
  }
]
